{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuCWhaY8KJ_i"
      },
      "source": [
        "# MLC 作业 1: 端到端模型执行"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_Xe61FhKJ_l"
      },
      "source": [
        "## 第一部分: 模型准备\n",
        "\n",
        "本作业的目标是让你对机器学习编译过程中的端到端模型的执行和变换更加熟悉。让我们从一个简单的图像分类模型开始。\n",
        "\n",
        "我们首先使用如下的命令来安装必要的库。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFFfd9q2KJ_m",
        "outputId": "2cdb94b8-ae24-46b6-aea6-f1822ad5100d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://mlc.ai/wheels\n",
            "Collecting mlc-ai-nightly\n",
            "  Downloading https://github.com/mlc-ai/utils/releases/download/v0.9.dev0/mlc_ai_nightly-0.9.dev1956%2Bge3f218d71-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (44.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 44.2 MB 2.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pygments in /usr/local/lib/python3.7/dist-packages (from mlc-ai-nightly) (2.6.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from mlc-ai-nightly) (5.4.8)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from mlc-ai-nightly) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mlc-ai-nightly) (1.21.6)\n",
            "Collecting synr==0.6.0\n",
            "  Downloading synr-0.6.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from mlc-ai-nightly) (22.1.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.7/dist-packages (from mlc-ai-nightly) (5.1.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from mlc-ai-nightly) (1.7.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from mlc-ai-nightly) (4.4.2)\n",
            "Installing collected packages: synr, mlc-ai-nightly\n",
            "Successfully installed mlc-ai-nightly-0.9.dev1956+ge3f218d71 synr-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.pytorch.org/whl/cpu\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.0+cu113)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.12.0+cu113)\n",
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pip install mlc-ai-nightly -f https://mlc.ai/wheels\n",
        "!python3 -m pip install torch torchvision torchaudio torchsummary --extra-index-url https://download.pytorch.org/whl/cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fZb8eLg7KJ_n"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle as pkl\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import tvm\n",
        "import tvm.testing\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "from tvm import topi, relax, te\n",
        "from tvm.script import tir as T\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8z3rJYxKJ_o"
      },
      "source": [
        "以下是用PyTorch定义的模型。该模型接受一批图像为输入，然后对它们依次作用卷积层，激活层，池化层和全连接层，得到分类结果。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "06uPam10KJ_o"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "input_shape = (batch_size, 1, 28, 28)  # NCHW layout\n",
        "\n",
        "\n",
        "def pytorch_model():\n",
        "    list = []\n",
        "    list.append(nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 3), bias=True))\n",
        "    list.append(nn.ReLU())\n",
        "    list.append(nn.MaxPool2d(kernel_size=(2, 2)))\n",
        "    list.append(nn.Flatten())\n",
        "    list.append(nn.Linear(in_features=5408, out_features=100, bias=True))\n",
        "    list.append(nn.ReLU())\n",
        "    list.append(nn.Linear(in_features=100, out_features=10, bias=True))\n",
        "    list.append(nn.Softmax(dim=1))\n",
        "\n",
        "    model = nn.Sequential(*list).cpu()\n",
        "    name_map = {\n",
        "        \"0.weight\": \"conv2d_weight\",\n",
        "        \"0.bias\": \"conv2d_bias\",\n",
        "        \"4.weight\": \"linear0_weight\",\n",
        "        \"4.bias\": \"linear0_bias\",\n",
        "        \"6.weight\": \"linear1_weight\",\n",
        "        \"6.bias\": \"linear1_bias\",\n",
        "    }\n",
        "    for name, param in model.named_parameters():\n",
        "        param.data = torch.from_numpy(weight_map[name_map[name]]).cpu()\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agdD9_zxKJ_p"
      },
      "source": [
        "我们提供了一个在Fashion MNIST数据集上的预训练权重图。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3UFdpcBKJ_q",
        "outputId": "d545c185-fd98-499c-e8fc-22e2632b43ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-06 08:03:26--  https://github.com/mlc-ai/web-data/raw/main/models/fasionmnist_mlp_assignment_params.pkl\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/mlc-ai/web-data/main/models/fasionmnist_mlp_assignment_params.pkl [following]\n",
            "--2022-08-06 08:03:27--  https://raw.githubusercontent.com/mlc-ai/web-data/main/models/fasionmnist_mlp_assignment_params.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2169350 (2.1M) [application/octet-stream]\n",
            "Saving to: ‘fasionmnist_mlp_assignment_params.pkl’\n",
            "\n",
            "fasionmnist_mlp_ass 100%[===================>]   2.07M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2022-08-06 08:03:27 (28.0 MB/s) - ‘fasionmnist_mlp_assignment_params.pkl’ saved [2169350/2169350]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Hide outputs\n",
        "!wget -nc https://github.com/mlc-ai/web-data/raw/main/models/fasionmnist_mlp_assignment_params.pkl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKr26ivxKJ_q"
      },
      "source": [
        "我们可以看到它的准确率约为84%。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "owc7WMEEKJ_r",
        "outputId": "0ed4f4ae-99b7-4468-ec42-ee188128719c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predict: Ankle boot, label: Ankle boot\n",
            "\n",
            "Test set: Average loss: -0.8369, Accuracy: 8388/10000 (84%)\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Load the weight map from file.\n",
        "# The prediction accuracy of the weight map on test data is around 83.3%.\n",
        "weight_map = pkl.load(open(\"fasionmnist_mlp_assignment_params.pkl\", \"rb\"))\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "\n",
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        print_img = True\n",
        "        for data, label in test_loader:\n",
        "            data, label = data.cpu(), label.cpu()\n",
        "            output = model(data)\n",
        "            # sum up batch loss\n",
        "            test_loss += F.nll_loss(output, label, reduction=\"sum\").item()\n",
        "            # get the index of the max log-probability\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            if print_img:\n",
        "                imshow(data[0])\n",
        "                print(\"predict: {}, label: {}\".format(class_names[pred[0][0]], class_names[label[0]]))\n",
        "                print_img = False\n",
        "            correct += pred.eq(label.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print(\"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5\n",
        "    npimg = img.numpy()\n",
        "    # plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    # plt.show()\n",
        "    img = img.reshape(1, 28, 28).numpy()\n",
        "    plt.figure()\n",
        "    plt.imshow(img[0]) \n",
        "\n",
        "\n",
        "test_data = torchvision.datasets.FashionMNIST(\n",
        "    \"./data\",\n",
        "    download=True,\n",
        "    train=False,\n",
        "    transform=transforms.Compose([transforms.ToTensor()])\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_data, batch_size=batch_size, shuffle=False)\n",
        "test(pytorch_model(), test_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jvu1WuJXKJ_r"
      },
      "source": [
        "## 第二部分: 从PyTorch迁移模型\n",
        "为了展示机器学习编译对端到端模型的抽象，我们需要将模型从PyTorch迁移并转换为TVMScript实现。然后，手工迁移很难。正如你在TensorIR练习中所体验的那样，为模型中的每一层写一个元张量函数需要大量的人力来完成。另外，手工写这些函数是容易犯错的。你可以想象，当你写了几百行，但其中有零星几个bug，那么找到bug的过程将会是痛苦的。\n",
        "\n",
        "幸运的是，在TVM中有一个简单的多的方法能够迁移模型。TVM提供了一个类`relax.BlockBuilder`，它能够从空白的IRModule开始一步步的构建端到端模型。（回忆我们在第四节课中介绍的Relax的Dataflow Block，这里的\"block\"就是代表了Relax函数中的Dataflow Block）\n",
        "\n",
        "具体而言，在 `BlockBuilder`中我们有一个 `emit_te`的API，它可以将一个张量表达式（第三节课中介绍过）的算子描述转变成一个对应TensorIR函数的`call_tir`操作（`call_tir`在第四节课中介绍过）。与手工写TensorIR函数相比，写张量表达式描述可以用几行代码来完成，这减少了需要的工作量和犯错的概率。\n",
        "\n",
        "`emit_te`的函数签名是`emit_te(func, *input)`，其中`func`是一个返回张量表达式的函数，而`*input`是`func`的输入。\n",
        "\n",
        "让我们从一个例子开始详细介绍。在下方的代码块中，`relu`是一个返回ReLU算子的张量表达式描述的函数。为了构建一个执行单个ReLU算子的Relax函数，在`emit_te_example`中我们首先定义了一个`BlockBuilder`实例`bb`。我们也定义了一个2维128x128大小的张量变量`x`，它将作为ReLU操作的输入张量（同时也是Relax函数的输入）。\n",
        "\n",
        "在这之后，我们用`with bb.function(name, [*input])` API构建一个以`x`为输入的Relax函数 `main`。然后我们构建一个dataflow block。在这个dataflow block里，我们首先用`emit_te`生成一个调用ReLU算子的`call_tir`。这里 `emit_te`在IRModule中生成了一个名字为`relu`的TensorIR函数，然后在dataflow block中生成`call_tir(relu, (x,), (128, 128), dtype=\"float32\")`操作。`call_tir`之后是函数返回。\n",
        "\n",
        "在这一构造之后，BlockBuilder实例`bb`包含构建完的IRModule，它可以通过`bb.get()`得到。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oCbyRJv3KJ_s"
      },
      "outputs": [],
      "source": [
        "def relu(A):\n",
        "    B = te.compute(shape=(128, 128), fcompute=lambda i, j: te.max(A[i, j], 0), name=\"B\")\n",
        "    return B\n",
        "\n",
        "\n",
        "def emit_te_example():\n",
        "    bb = relax.BlockBuilder()\n",
        "    x = relax.Var(\"x\", (128, 128), relax.DynTensorType(2, \"float32\"))\n",
        "    with bb.function(\"main\", [x]):\n",
        "        with bb.dataflow():\n",
        "            lv0 = bb.emit_te(relu, x)\n",
        "            gv = bb.emit_output(lv0)\n",
        "        bb.emit_func_output(gv)\n",
        "    return bb.get()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRMUxdkCKJ_s"
      },
      "source": [
        "函数`emit_te_example`返回构造得到的IRModule。为了看的更清楚，我们可以输出这一IRModule。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBE1VNfeKJ_t",
        "outputId": "5177705b-bab0-4a67-e55c-23274d8fcd37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@tvm.script.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def relu(rxplaceholder: T.Buffer[(128, 128), \"float32\"], B: T.Buffer[(128, 128), \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"relu\", \"tir.noalias\": True})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        for i0, i1 in T.grid(128, 128):\n",
            "            with T.block(\"B\"):\n",
            "                i, j = T.axis.remap(\"SS\", [i0, i1])\n",
            "                T.reads(rxplaceholder[i, j])\n",
            "                T.writes(B[i, j])\n",
            "                B[i, j] = T.max(rxplaceholder[i, j], T.float32(0))\n",
            "    \n",
            "    @R.function\n",
            "    def main(x: Tensor((128, 128), \"float32\")) -> Tensor(None, \"float32\", ndim = 2):\n",
            "        # block 0\n",
            "        with R.dataflow():\n",
            "            lv = R.call_tir(relu, (x,), (128, 128), dtype=\"float32\")\n",
            "            gv: Tensor((128, 128), \"float32\") = lv\n",
            "            R.output(gv)\n",
            "        return gv\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "#import IPython\n",
        "\n",
        "#mod = emit_te_example()\n",
        "#IPython.display.Code(mod.script(), language=\"python\")\n",
        "\n",
        "import IPython\n",
        "\n",
        "def code2html(code):\n",
        "    \"\"\"Helper function to use pygments to turn the code string into highlighted html.\"\"\"\n",
        "    import pygments\n",
        "    from pygments.formatters import HtmlFormatter\n",
        "    from pygments.lexers import Python3Lexer\n",
        "    formatter = HtmlFormatter()\n",
        "    html = pygments.highlight(code, Python3Lexer(), formatter)\n",
        "    return \"<style>%s</style>%s\\n\" % (formatter.get_style_defs(\".highlight\"), html)\n",
        "\n",
        "mod = emit_te_example()\n",
        "print(mod.script())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq9ktpOgKJ_t"
      },
      "source": [
        "正如你看到的，通过BlockBuilder生成的IRModule确实包含了ReLU的TensorIR实现和一个含有调用ReLU实现的`call_tir`的Relax函数\n",
        "\n",
        "现在轮到你来用BlockBuilder和`emit_te`来创建一个和之前定义的PyTorch模型等价的IRModule。你可以自己为所有的算子写张量表达式描述。或者，TVM提供了TOPI（TVM Operator Inventory）库，它为不同的算子提供了张量表达式描述。如果你愿意阅读[文档](https://tvm.apache.org/docs/reference/api/python/topi.html)来弄懂它的用法，这也是被鼓励的。我们提供了测试函数来检查你的IRModule的正确性。\n",
        "\n",
        "注意到每个Conv2d层和linear层都包含了一个偏置加法，这应该在你构建的IRModule中被体现。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91uCLPkPKJ_t",
        "outputId": "4b0d0b8e-4ad2-4879-de3d-8dd0998b5e74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@tvm.script.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def relu(rxplaceholder: T.Buffer[(4, 32, 26, 26), \"float32\"], compute: T.Buffer[(4, 32, 26, 26), \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"relu\", \"tir.noalias\": True})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        for i0, i1, i2, i3 in T.grid(4, 32, 26, 26):\n",
            "            with T.block(\"compute\"):\n",
            "                i0_1, i1_1, i2_1, i3_1 = T.axis.remap(\"SSSS\", [i0, i1, i2, i3])\n",
            "                T.reads(rxplaceholder[i0_1, i1_1, i2_1, i3_1])\n",
            "                T.writes(compute[i0_1, i1_1, i2_1, i3_1])\n",
            "                compute[i0_1, i1_1, i2_1, i3_1] = T.max(rxplaceholder[i0_1, i1_1, i2_1, i3_1], T.float32(0))\n",
            "    \n",
            "    @T.prim_func\n",
            "    def add2(rxplaceholder: T.Buffer[(4, 10), \"float32\"], rxplaceholder_1: T.Buffer[(1, 10), \"float32\"], T_add: T.Buffer[(4, 10), \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"add2\", \"tir.noalias\": True})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        for i0, i1 in T.grid(4, 10):\n",
            "            with T.block(\"T_add\"):\n",
            "                ax0, ax1 = T.axis.remap(\"SS\", [i0, i1])\n",
            "                T.reads(rxplaceholder[ax0, ax1], rxplaceholder_1[0, ax1])\n",
            "                T.writes(T_add[ax0, ax1])\n",
            "                T_add[ax0, ax1] = rxplaceholder[ax0, ax1] + rxplaceholder_1[0, ax1]\n",
            "    \n",
            "    @T.prim_func\n",
            "    def add1(rxplaceholder: T.Buffer[(4, 100), \"float32\"], rxplaceholder_1: T.Buffer[(1, 100), \"float32\"], T_add: T.Buffer[(4, 100), \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"add1\", \"tir.noalias\": True})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        for i0, i1 in T.grid(4, 100):\n",
            "            with T.block(\"T_add\"):\n",
            "                ax0, ax1 = T.axis.remap(\"SS\", [i0, i1])\n",
            "                T.reads(rxplaceholder[ax0, ax1], rxplaceholder_1[0, ax1])\n",
            "                T.writes(T_add[ax0, ax1])\n",
            "                T_add[ax0, ax1] = rxplaceholder[ax0, ax1] + rxplaceholder_1[0, ax1]\n",
            "    \n",
            "    @T.prim_func\n",
            "    def conv2d(rxplaceholder: T.Buffer[(4, 1, 28, 28), \"float32\"], rxplaceholder_1: T.Buffer[(32, 1, 3, 3), \"float32\"], conv2d_nchw: T.Buffer[(4, 32, 26, 26), \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"conv2d\", \"tir.noalias\": True})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        pad_temp = T.alloc_buffer([4, 1, 28, 28], dtype=\"float32\")\n",
            "        for i0, i1, i2, i3 in T.grid(4, 1, 28, 28):\n",
            "            with T.block(\"pad_temp\"):\n",
            "                i0_1, i1_1, i2_1, i3_1 = T.axis.remap(\"SSSS\", [i0, i1, i2, i3])\n",
            "                T.reads(rxplaceholder[i0_1, i1_1, i2_1, i3_1])\n",
            "                T.writes(pad_temp[i0_1, i1_1, i2_1, i3_1])\n",
            "                pad_temp[i0_1, i1_1, i2_1, i3_1] = rxplaceholder[i0_1, i1_1, i2_1, i3_1]\n",
            "        for i0, i1, i2, i3, i4, i5, i6 in T.grid(4, 32, 26, 26, 1, 3, 3):\n",
            "            with T.block(\"conv2d_nchw\"):\n",
            "                nn, ff, yy, xx, rc, ry, rx = T.axis.remap(\"SSSSRRR\", [i0, i1, i2, i3, i4, i5, i6])\n",
            "                T.reads(pad_temp[nn, rc, yy + ry, xx + rx], rxplaceholder_1[ff, rc, ry, rx])\n",
            "                T.writes(conv2d_nchw[nn, ff, yy, xx])\n",
            "                with T.init():\n",
            "                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)\n",
            "                conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + pad_temp[nn, rc, yy + ry, xx + rx] * rxplaceholder_1[ff, rc, ry, rx]\n",
            "    \n",
            "    @T.prim_func\n",
            "    def softmax(rxplaceholder: T.Buffer[(4, 10), \"float32\"], T_softmax_norm: T.Buffer[(4, 10), \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"softmax\", \"tir.noalias\": True})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        T_softmax_maxelem = T.alloc_buffer([4], dtype=\"float32\")\n",
            "        T_softmax_exp = T.alloc_buffer([4, 10], dtype=\"float32\")\n",
            "        T_softmax_expsum = T.alloc_buffer([4], dtype=\"float32\")\n",
            "        for i0, i1 in T.grid(4, 10):\n",
            "            with T.block(\"T_softmax_maxelem\"):\n",
            "                i0_1, k = T.axis.remap(\"SR\", [i0, i1])\n",
            "                T.reads(rxplaceholder[i0_1, k])\n",
            "                T.writes(T_softmax_maxelem[i0_1])\n",
            "                with T.init():\n",
            "                    T_softmax_maxelem[i0_1] = T.float32(-3.4028234663852886e+38)\n",
            "                T_softmax_maxelem[i0_1] = T.max(T_softmax_maxelem[i0_1], rxplaceholder[i0_1, k])\n",
            "        for i0, i1 in T.grid(4, 10):\n",
            "            with T.block(\"T_softmax_exp\"):\n",
            "                i0_2, i1_1 = T.axis.remap(\"SS\", [i0, i1])\n",
            "                T.reads(rxplaceholder[i0_2, i1_1], T_softmax_maxelem[i0_2])\n",
            "                T.writes(T_softmax_exp[i0_2, i1_1])\n",
            "                T_softmax_exp[i0_2, i1_1] = T.exp(rxplaceholder[i0_2, i1_1] - T_softmax_maxelem[i0_2], dtype=\"float32\")\n",
            "        for i0_3, i1 in T.grid(4, 10):\n",
            "            with T.block(\"T_softmax_expsum\"):\n",
            "                i0_4, k = T.axis.remap(\"SR\", [i0_3, i1])\n",
            "                T.reads(T_softmax_exp[i0_4, k])\n",
            "                T.writes(T_softmax_expsum[i0_4])\n",
            "                with T.init():\n",
            "                    T_softmax_expsum[i0_4] = T.float32(0)\n",
            "                T_softmax_expsum[i0_4] = T_softmax_expsum[i0_4] + T_softmax_exp[i0_4, k]\n",
            "        for i0_5, i1 in T.grid(4, 10):\n",
            "            with T.block(\"T_softmax_norm\"):\n",
            "                i0_6, i1_2 = T.axis.remap(\"SS\", [i0_5, i1])\n",
            "                T.reads(T_softmax_exp[i0_6, i1_2], T_softmax_expsum[i0_6])\n",
            "                T.writes(T_softmax_norm[i0_6, i1_2])\n",
            "                T.block_attr({\"axis\":1})\n",
            "                T_softmax_norm[i0_6, i1_2] = T_softmax_exp[i0_6, i1_2] / T_softmax_expsum[i0_6]\n",
            "    \n",
            "    @R.function\n",
            "    def main(x: Tensor((4, 1, 28, 28), \"float32\")) -> Tensor(None, \"float32\", ndim = 2):\n",
            "        # block 0\n",
            "        with R.dataflow():\n",
            "            lv = R.call_tir(conv2d, (x, meta[relay.Constant][0]), (4, 32, 26, 26), dtype=\"float32\")\n",
            "            lv1 = R.call_tir(add, (lv, meta[relay.Constant][1]), (4, 32, 26, 26), dtype=\"float32\")\n",
            "            lv2 = R.call_tir(relu, (lv1,), (4, 32, 26, 26), dtype=\"float32\")\n",
            "            lv3 = R.call_tir(pool2d, (lv2,), (4, 32, 13, 13), dtype=\"float32\")\n",
            "            lv4 = R.call_tir(flatten, (lv3,), (4, 5408), dtype=\"float32\")\n",
            "            lv5 = R.call_tir(dense, (lv4, meta[relay.Constant][2]), (4, 100), dtype=\"float32\")\n",
            "            lv6 = R.call_tir(add1, (lv5, meta[relay.Constant][3]), (4, 100), dtype=\"float32\")\n",
            "            lv7 = R.call_tir(relu1, (lv6,), (4, 100), dtype=\"float32\")\n",
            "            lv8 = R.call_tir(dense1, (lv7, meta[relay.Constant][4]), (4, 10), dtype=\"float32\")\n",
            "            lv9 = R.call_tir(add2, (lv8, meta[relay.Constant][5]), (4, 10), dtype=\"float32\")\n",
            "            lv10 = R.call_tir(softmax, (lv9,), (4, 10), dtype=\"float32\")\n",
            "            gv: Tensor((4, 10), \"float32\") = lv10\n",
            "            R.output(gv)\n",
            "        return gv\n",
            "    \n",
            "    @T.prim_func\n",
            "    def flatten(rxplaceholder: T.Buffer[(4, 32, 13, 13), \"float32\"], compute: T.Buffer[(4, 5408), \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"flatten\", \"tir.noalias\": True})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        for i0, i1 in T.grid(4, 5408):\n",
            "            with T.block(\"compute\"):\n",
            "                i, j = T.axis.remap(\"SS\", [i0, i1])\n",
            "                T.reads(rxplaceholder[i, j % 5408 // 169, j % 169 // 13, j % 13])\n",
            "                T.writes(compute[i, j])\n",
            "                compute[i, j] = rxplaceholder[i, j % 5408 // 169, j % 169 // 13, j % 13]\n",
            "    \n",
            "    @T.prim_func\n",
            "    def dense1(rxplaceholder: T.Buffer[(4, 100), \"float32\"], rxplaceholder_1: T.Buffer[(10, 100), \"float32\"], T_matmul_NT: T.Buffer[(4, 10), \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"dense1\", \"tir.noalias\": True, \"layout_free_buffers\": [1]})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        for i0, i1, i2 in T.grid(4, 10, 100):\n",
            "            with T.block(\"T_matmul_NT\"):\n",
            "                i, j, k = T.axis.remap(\"SSR\", [i0, i1, i2])\n",
            "                T.reads(rxplaceholder[i, k], rxplaceholder_1[j, k])\n",
            "                T.writes(T_matmul_NT[i, j])\n",
            "                with T.init():\n",
            "                    T_matmul_NT[i, j] = T.float32(0)\n",
            "                T_matmul_NT[i, j] = T_matmul_NT[i, j] + rxplaceholder[i, k] * rxplaceholder_1[j, k]\n",
            "    \n",
            "    @T.prim_func\n",
            "    def relu1(rxplaceholder: T.Buffer[(4, 100), \"float32\"], compute: T.Buffer[(4, 100), \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"relu1\", \"tir.noalias\": True})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        for i0, i1 in T.grid(4, 100):\n",
            "            with T.block(\"compute\"):\n",
            "                i0_1, i1_1 = T.axis.remap(\"SS\", [i0, i1])\n",
            "                T.reads(rxplaceholder[i0_1, i1_1])\n",
            "                T.writes(compute[i0_1, i1_1])\n",
            "                compute[i0_1, i1_1] = T.max(rxplaceholder[i0_1, i1_1], T.float32(0))\n",
            "    \n",
            "    @T.prim_func\n",
            "    def pool2d(rxplaceholder: T.Buffer[(4, 32, 26, 26), \"float32\"], tensor: T.Buffer[(4, 32, 13, 13), \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"pool2d\", \"tir.noalias\": True})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        for i0, i1, i2, i3, i4, i5 in T.grid(4, 32, 13, 13, 2, 2):\n",
            "            with T.block(\"tensor\"):\n",
            "                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap(\"SSSSRR\", [i0, i1, i2, i3, i4, i5])\n",
            "                T.reads(rxplaceholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])\n",
            "                T.writes(tensor[ax0, ax1, ax2, ax3])\n",
            "                with T.init():\n",
            "                    tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)\n",
            "                tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], rxplaceholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])\n",
            "    \n",
            "    @T.prim_func\n",
            "    def add(rxplaceholder: T.Buffer[(4, 32, 26, 26), \"float32\"], rxplaceholder_1: T.Buffer[(1, 32, 1, 1), \"float32\"], T_add: T.Buffer[(4, 32, 26, 26), \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"add\", \"tir.noalias\": True})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        for i0, i1, i2, i3 in T.grid(4, 32, 26, 26):\n",
            "            with T.block(\"T_add\"):\n",
            "                ax0, ax1, ax2, ax3 = T.axis.remap(\"SSSS\", [i0, i1, i2, i3])\n",
            "                T.reads(rxplaceholder[ax0, ax1, ax2, ax3], rxplaceholder_1[0, ax1, 0, 0])\n",
            "                T.writes(T_add[ax0, ax1, ax2, ax3])\n",
            "                T_add[ax0, ax1, ax2, ax3] = rxplaceholder[ax0, ax1, ax2, ax3] + rxplaceholder_1[0, ax1, 0, 0]\n",
            "    \n",
            "    @T.prim_func\n",
            "    def dense(rxplaceholder: T.Buffer[(4, 5408), \"float32\"], rxplaceholder_1: T.Buffer[(100, 5408), \"float32\"], T_matmul_NT: T.Buffer[(4, 100), \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"dense\", \"tir.noalias\": True, \"layout_free_buffers\": [1]})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        for i0, i1, i2 in T.grid(4, 100, 5408):\n",
            "            with T.block(\"T_matmul_NT\"):\n",
            "                i, j, k = T.axis.remap(\"SSR\", [i0, i1, i2])\n",
            "                T.reads(rxplaceholder[i, k], rxplaceholder_1[j, k])\n",
            "                T.writes(T_matmul_NT[i, j])\n",
            "                with T.init():\n",
            "                    T_matmul_NT[i, j] = T.float32(0)\n",
            "                T_matmul_NT[i, j] = T_matmul_NT[i, j] + rxplaceholder[i, k] * rxplaceholder_1[j, k]\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "def create_model_via_emit_te():\n",
        "    bb = relax.BlockBuilder()\n",
        "    x = relax.Var(\"x\", input_shape, relax.DynTensorType(batch_size, \"float32\"))\n",
        "\n",
        "    conv2d_weight = relax.const(weight_map[\"conv2d_weight\"], \"float32\")\n",
        "    conv2d_bias = relax.const(weight_map[\"conv2d_bias\"].reshape(1, 32, 1, 1), \"float32\")\n",
        "    linear0_weight = relax.const(weight_map[\"linear0_weight\"], \"float32\")\n",
        "    linear0_bias = relax.const(weight_map[\"linear0_bias\"].reshape(1, 100), \"float32\")\n",
        "    linear1_weight = relax.const(weight_map[\"linear1_weight\"], \"float32\")\n",
        "    linear1_bias = relax.const(weight_map[\"linear1_bias\"].reshape(1, 10), \"float32\")\n",
        "\n",
        "    with bb.function(\"main\", [x]):\n",
        "        with bb.dataflow():\n",
        "           '''\n",
        "            list.append(nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 3), bias=True))\n",
        "            list.append(nn.ReLU())\n",
        "            list.append(nn.MaxPool2d(kernel_size=(2, 2)))\n",
        "            list.append(nn.Flatten())\n",
        "            list.append(nn.Linear(in_features=5408, out_features=100, bias=True))\n",
        "            list.append(nn.ReLU())\n",
        "            list.append(nn.Linear(in_features=100, out_features=10, bias=True))\n",
        "            list.append(nn.Softmax(dim=1))\n",
        "           '''\n",
        "           # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
        "           lv0 = bb.emit_te(topi.nn.conv2d, x, conv2d_weight, 1, 0, 1)\n",
        "           lv1 = bb.emit_te(topi.add, lv0, conv2d_bias)\n",
        "           lv2 = bb.emit_te(topi.nn.relu, lv1)\n",
        "           # tvm.topi.nn.pool2d(data, kernel, stride, dilation, padding, pool_type, ceil_mode=False, layout='NCHW', count_include_pad=True)\n",
        "           lv3 = bb.emit_te(topi.nn.pool2d, lv2, [2,2], [2,2], [1,1], [0,0,0,0], 'max')\n",
        "           lv4 = bb.emit_te(topi.nn.flatten, lv3)\n",
        "           lv5 = bb.emit_te(topi.nn.dense, lv4, linear0_weight)\n",
        "           lv50 = bb.emit_te(topi.add, lv5, linear0_bias)\n",
        "           lv6 = bb.emit_te(topi.nn.relu, lv50)\n",
        "           \n",
        "           lv7 = bb.emit_te(topi.nn.dense, lv6, linear1_weight)\n",
        "           lv70 = bb.emit_te(topi.add, lv7, linear1_bias)\n",
        "           lv8 = bb.emit_te(topi.nn.softmax, lv70)\n",
        "           gv = bb.emit_output(lv8)\n",
        "        bb.emit_func_output(gv)\n",
        "\n",
        "    return bb.get()\n",
        "\n",
        "\n",
        "def build_mod(mod):\n",
        "    exec = relax.vm.build(mod, \"llvm\")\n",
        "    dev = tvm.cpu()\n",
        "    vm = relax.VirtualMachine(exec, dev)\n",
        "    return vm\n",
        "\n",
        "\n",
        "def check_equivalence(mod, torch_model, test_loader):\n",
        "    torch_model.eval()\n",
        "    with torch.no_grad():\n",
        "        rt_mod = build_mod(mod)\n",
        "        for data, label in test_loader:\n",
        "            data, label = data.cpu(), label.cpu()\n",
        "            output_from_pytorch = torch_model(data).numpy()\n",
        "            output_from_relax = rt_mod[\"main\"](tvm.nd.array(data, tvm.cpu())).numpy()\n",
        "            tvm.testing.assert_allclose(output_from_pytorch, output_from_relax, rtol=1e-4)\n",
        "\n",
        "\n",
        "test_data = torchvision.datasets.FashionMNIST(\n",
        "    \"./data\",\n",
        "    download=True,\n",
        "    train=False,\n",
        "    transform=transforms.Compose([transforms.ToTensor()])\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "mod = create_model_via_emit_te()\n",
        "torch_model = pytorch_model()\n",
        "\n",
        "check_equivalence(mod, torch_model, test_loader)\n",
        "# IPython.display.Code(mod.script(), language=\"python\")\n",
        "print(mod.script())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmtBA7t6KJ_u"
      },
      "source": [
        "## 第三部分: 使用库\n",
        "\n",
        "正如我们在第四节课中谈到的，我们可以将torch函数整合进IRModule。步骤包括注册一个外部运行时函数，和在IRModule中用`call_tir`调用。\n",
        "\n",
        "这里是一个用torch matmul和torch add拉力实现一个linear层的例子。你也可以在第四节课的笔记中找到这个例子。\n",
        "\n",
        "\n",
        "```python\n",
        "@tvm.register_func(\"env.linear\", override=True)\n",
        "def torch_linear(x: tvm.nd.NDArray,\n",
        "                 w: tvm.nd.NDArray,\n",
        "                 b: tvm.nd.NDArray,\n",
        "                 out: tvm.nd.NDArray):\n",
        "    x_torch = torch.from_dlpack(x)\n",
        "    w_torch = torch.from_dlpack(w)\n",
        "    b_torch = torch.from_dlpack(b)\n",
        "    out_torch = torch.from_dlpack(out)\n",
        "    torch.mm(x_torch, w_torch.T, out=out_torch)\n",
        "    torch.add(out_torch, b_torch, out=out_torch)\n",
        "\n",
        "\n",
        "@tvm.script.ir_module\n",
        "class MyModuleWithExternCall:\n",
        "    @R.function\n",
        "    def main(x: Tensor((1, 784), \"float32\"),\n",
        "             w0: Tensor((128, 784), \"float32\"),\n",
        "             b0: Tensor((128,), \"float32\")):\n",
        "        # block 0\n",
        "        with R.dataflow():\n",
        "            lv0 = R.call_tir(\"env.linear\", (x, w0, b0), (1, 128), dtype=\"float32\")\n",
        "            ...\n",
        "        return ...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bf8QGNHKKJ_u"
      },
      "source": [
        "请为你在第二部分中创建的IRModule中的卷积层注册外部函数。你需要使用NumPy或者PyTorch作为你的函数实现。\n",
        "\n",
        "你可能需要使用`BlockBuilder.emit`在正在构建的Relax函数的结尾直接添加一个`call_tir`操作。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "WTcmQE7yKJ_u"
      },
      "outputs": [],
      "source": [
        "@tvm.register_func(\"env.conv2d\", override=True)\n",
        "def pytorch_conv2d(x: tvm.nd.NDArray, w: tvm.nd.NDArray, out : tvm.nd.NDArray):\n",
        "    x_torch = torch.from_dlpack(x)\n",
        "    w_torch = torch.from_dlpack(w)\n",
        "    out_torch = torch.from_dlpack(out)\n",
        "    t = torch.nn.functional.conv2d(x_torch, w_torch)\n",
        "    out_torch.copy_(t) # https://github.com/mlc-ai/mlc-en/discussions/31#discussioncomment-3273060\n",
        "\n",
        "def create_model_with_torch_func():\n",
        "    bb = relax.BlockBuilder()\n",
        "\n",
        "    x = relax.Var(\"x\", input_shape, relax.DynTensorType(4, \"float32\"))\n",
        "\n",
        "    conv2d_weight = relax.const(weight_map[\"conv2d_weight\"], \"float32\")\n",
        "    conv2d_bias = relax.const(weight_map[\"conv2d_bias\"].reshape(1, 32, 1, 1), \"float32\")\n",
        "    linear0_weight = relax.const(weight_map[\"linear0_weight\"], \"float32\")\n",
        "    linear0_bias = relax.const(weight_map[\"linear0_bias\"].reshape(1, 100), \"float32\")\n",
        "    linear1_weight = relax.const(weight_map[\"linear1_weight\"], \"float32\")\n",
        "    linear1_bias = relax.const(weight_map[\"linear1_bias\"].reshape(1, 10), \"float32\")\n",
        "\n",
        "    with bb.function(\"main\", [x]):\n",
        "        with bb.dataflow():\n",
        "            lv0 = bb.emit(relax.op.call_tir(relax.extern(\"env.conv2d\"), (x, conv2d_weight), (4, 32, 26, 26), dtype=\"float32\"))\n",
        "            # Same as before\n",
        "            lv1 = bb.emit_te(topi.add, lv0, conv2d_bias)\n",
        "            lv2 = bb.emit_te(topi.nn.relu, lv1)\n",
        "            lv3 = bb.emit_te(topi.nn.pool2d, lv2, [2,2], [2,2], [1,1], [0,0,0,0], 'max')\n",
        "            lv4 = bb.emit_te(topi.nn.flatten, lv3)\n",
        "            lv5 = bb.emit_te(topi.nn.dense, lv4, linear0_weight)\n",
        "            lv50 = bb.emit_te(topi.add, lv5, linear0_bias)\n",
        "            lv6 = bb.emit_te(topi.nn.relu, lv50)\n",
        "            \n",
        "            lv7 = bb.emit_te(topi.nn.dense, lv6, linear1_weight)\n",
        "            lv70 = bb.emit_te(topi.add, lv7, linear1_bias)\n",
        "            lv8 = bb.emit_te(topi.nn.softmax, lv70)\n",
        "            gv = bb.emit_output(lv8)\n",
        "        bb.emit_func_output(gv)\n",
        "\n",
        "    return bb.get()\n",
        "\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "mod = create_model_with_torch_func()\n",
        "check_equivalence(mod, torch_model, test_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwbWjCGcKJ_u"
      },
      "source": [
        "## 第四部分: 端到端模型中的程序变换\n",
        "\n",
        "在TensorIR练习中, 我们学会了如何变换单个TensorIR函数。在端到端模型中变换是类似的。\n",
        "\n",
        "和批量矩阵乘法相比，让我们关注一个更加有挑战性的算子：conv2d（二维卷积）。\n",
        "\n",
        "首先，让我们介绍一些新的原语：\n",
        " - `compute_inline`：它将一个block内联到另一个block中，以减少内存使用大小和内存访问次数\n",
        " - `fuse`：和`split`相对。融合多个轴。这里`fuse`与`parallel` / `vectorize` / `unroll`一起使用，以增加并行度。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tds5dPuMKJ_v",
        "outputId": "e89fd723-88e4-45f8-b7ff-6f55835d27f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# from tvm.script import tir as T\n",
            "@T.prim_func\n",
            "def func(A: T.Buffer[(128, 128), \"float32\"], C: T.Buffer[(128, 128), \"float32\"]) -> None:\n",
            "    # body\n",
            "    # with T.block(\"root\")\n",
            "    for i, j in T.grid(128, 128):\n",
            "        with T.block(\"C\"):\n",
            "            vi, vj = T.axis.remap(\"SS\", [i, j])\n",
            "            T.reads(A[vi, vj])\n",
            "            T.writes(C[vi, vj])\n",
            "            C[vi, vj] = A[vi, vj] * T.float32(2) + T.float32(1)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "@T.prim_func\n",
        "def before_inline(a: T.handle, c: T.handle) -> None:\n",
        "    A = T.match_buffer(a, (128, 128))\n",
        "    B = T.alloc_buffer((128, 128))\n",
        "    C = T.match_buffer(c, (128, 128))\n",
        "    for i, j in T.grid(128, 128):\n",
        "        with T.block(\"B\"):\n",
        "            vi, vj = T.axis.remap(\"SS\", [i, j])\n",
        "            B[vi, vj] = A[vi, vj] * 2.0\n",
        "    for i, j in T.grid(128, 128):\n",
        "        with T.block(\"C\"):\n",
        "            vi, vj = T.axis.remap(\"SS\", [i, j])\n",
        "            C[vi, vj] = B[vi, vj] + 1.0\n",
        "\n",
        "\n",
        "sch = tvm.tir.Schedule(before_inline)\n",
        "sch.compute_inline(sch.get_block(\"B\"))\n",
        "print(sch.mod[\"main\"].script())\n",
        "# IPython.display.Code(sch.mod[\"main\"].script(), language=\"python\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkeymXt6KJ_v",
        "outputId": "c261d823-10c2-435a-b515-4fd6b0519593"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# from tvm.script import tir as T\n",
            "@T.prim_func\n",
            "def func(A: T.Buffer[(128, 128), \"float32\"], B: T.Buffer[(128, 128), \"float32\"]) -> None:\n",
            "    # body\n",
            "    # with T.block(\"root\")\n",
            "    for i_j_fused in T.serial(16384):\n",
            "        with T.block(\"B\"):\n",
            "            vi = T.axis.spatial(128, i_j_fused // 128)\n",
            "            vj = T.axis.spatial(128, i_j_fused % 128)\n",
            "            T.reads(A[vi, vj])\n",
            "            T.writes(B[vi, vj])\n",
            "            B[vi, vj] = A[vi, vj] * T.float32(2)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "@T.prim_func\n",
        "def before_fuse(a: T.handle, b: T.handle) -> None:\n",
        "    A = T.match_buffer(a, (128, 128))\n",
        "    B = T.match_buffer(b, (128, 128))\n",
        "    for i, j in T.grid(128, 128):\n",
        "        with T.block(\"B\"):\n",
        "            vi, vj = T.axis.remap(\"SS\", [i, j])\n",
        "            B[vi, vj] = A[vi, vj] * 2.0\n",
        "\n",
        "\n",
        "sch = tvm.tir.Schedule(before_fuse)\n",
        "i, j = sch.get_loops(sch.get_block(\"B\"))\n",
        "sch.fuse(i, j)\n",
        "print(sch.mod[\"main\"].script())\n",
        "# IPython.display.Code(sch.mod[\"main\"].script(), language=\"python\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH1j0XgmKJ_v"
      },
      "source": [
        "现在我们首先为第二部分中得到的IRModule创建一个schedule，然后对其中的conv2d TensorIR函数变换。和TensorIR练习类似，我们提供了一个目标函数。但请注意，目标函数不是标准答案，原因如下：\n",
        " - 它可能不能在所有硬件中都取得最佳性能\n",
        " - 原始的conv2d TensorIR实现可能不同，这决定与你在第二部分中使用的张量表达式描述：\n",
        "   - 如果你将conv2d的计算和偏置加法的计算放在了一个张量表达式中，那么在变换完成的TensorIR函数的末尾应该有一个计算偏置加法的block\n",
        "   - 如果你将上述两个计算分开在不同的张量表达式，或者你使用了TOPI提供的conv2d，那么变换完成的TensorIR函数末尾不应该有计算偏置加法的block。下面给出的目标函数是用TOPI conv2d获得的TensorIR函数做变换后生成的。\n",
        "\n",
        "```python\n",
        "@T.prim_func\n",
        "def target_func(rxplaceholder: T.Buffer[(4, 1, 28, 28), \"float32\"], rxplaceholder_1: T.Buffer[(32, 1, 3, 3), \"float32\"], conv2d_nchw: T.Buffer[(4, 32, 26, 26), \"float32\"]) -> None:\n",
        "    T.func_attr({\"global_symbol\": \"conv2d\", \"tir.noalias\": True})\n",
        "    # body\n",
        "    # with T.block(\"root\")\n",
        "    for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(2704):\n",
        "        for i0_1_i1_1_fused_init in T.unroll(8):\n",
        "            for i2_1_i3_1_fused_init in T.vectorized(4):\n",
        "                with T.block(\"conv2d_nchw_init\"):\n",
        "                    nn = T.axis.spatial(\n",
        "                        4, i0_0_i1_0_i2_0_i3_0_fused // 1352 * 2 + i0_1_i1_1_fused_init // 4)\n",
        "                    ff = T.axis.spatial(\n",
        "                        32, i0_0_i1_0_i2_0_i3_0_fused % 1352 // 169 * 4 + i0_1_i1_1_fused_init % 4)\n",
        "                    yy = T.axis.spatial(\n",
        "                        26, i0_0_i1_0_i2_0_i3_0_fused % 169 // 13 * 2 + i2_1_i3_1_fused_init // 2)\n",
        "                    xx = T.axis.spatial(\n",
        "                        26, i0_0_i1_0_i2_0_i3_0_fused % 13 * 2 + i2_1_i3_1_fused_init % 2)\n",
        "                    T.reads()\n",
        "                    T.writes(conv2d_nchw[nn, ff, yy, xx])\n",
        "                    conv2d_nchw[nn, ff, yy, xx] = T.float32(0)\n",
        "        for i4, i5, i6 in T.grid(1, 3, 3):\n",
        "            for i0_1_i1_1_fused in T.unroll(8):\n",
        "                for i2_1_i3_1_fused in T.vectorized(4):\n",
        "                    with T.block(\"conv2d_nchw_update\"):\n",
        "                        nn = T.axis.spatial(\n",
        "                            4, i0_0_i1_0_i2_0_i3_0_fused // 1352 * 2 + i0_1_i1_1_fused // 4)\n",
        "                        ff = T.axis.spatial(\n",
        "                            32, i0_0_i1_0_i2_0_i3_0_fused % 1352 // 169 * 4 + i0_1_i1_1_fused % 4)\n",
        "                        yy = T.axis.spatial(\n",
        "                            26, i0_0_i1_0_i2_0_i3_0_fused % 169 // 13 * 2 + i2_1_i3_1_fused // 2)\n",
        "                        xx = T.axis.spatial(\n",
        "                            26, i0_0_i1_0_i2_0_i3_0_fused % 13 * 2 + i2_1_i3_1_fused % 2)\n",
        "                        rc, ry, rx = T.axis.remap(\"RRR\", [i4, i5, i6])\n",
        "                        T.reads(conv2d_nchw[nn, ff, yy, xx], rxplaceholder[nn,\n",
        "                                rc, yy + ry, xx + rx], rxplaceholder_1[ff, rc, ry, rx])\n",
        "                        T.writes(conv2d_nchw[nn, ff, yy, xx])\n",
        "                        conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + \\\n",
        "                            rxplaceholder[nn, rc, yy + ry, xx +\n",
        "                                          rx] * rxplaceholder_1[ff, rc, ry, rx]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kw-CEMbnKJ_w"
      },
      "source": [
        "和TensorIR练习中不同的是, 这里schedule是为一个IRModule创建的，而不是TensorIR函数. 因此，当使用`sch.get_block`时，需要提供TensorIR函数名字，如下方所示。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-M-uzJ2jKJ_w",
        "outputId": "c6f8294d-f19c-4253-e363-5a0c33377439"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "@tvm.script.ir_module\n",
            "class Module:\n",
            "    @T.prim_func\n",
            "    def dense1(rxplaceholder: T.Buffer[(4, 100), \"float32\"], rxplaceholder_1: T.Buffer[(10, 100), \"float32\"], T_matmul_NT: T.Buffer[(4, 10), \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"dense1\", \"tir.noalias\": True, \"layout_free_buffers\": [1]})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        for i0, i1, i2 in T.grid(4, 10, 100):\n",
            "            with T.block(\"T_matmul_NT\"):\n",
            "                i, j, k = T.axis.remap(\"SSR\", [i0, i1, i2])\n",
            "                T.reads(rxplaceholder[i, k], rxplaceholder_1[j, k])\n",
            "                T.writes(T_matmul_NT[i, j])\n",
            "                with T.init():\n",
            "                    T_matmul_NT[i, j] = T.float32(0)\n",
            "                T_matmul_NT[i, j] = T_matmul_NT[i, j] + rxplaceholder[i, k] * rxplaceholder_1[j, k]\n",
            "    \n",
            "    @T.prim_func\n",
            "    def add1(rxplaceholder: T.Buffer[(4, 100), \"float32\"], rxplaceholder_1: T.Buffer[(1, 100), \"float32\"], T_add: T.Buffer[(4, 100), \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"add1\", \"tir.noalias\": True})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        for i0, i1 in T.grid(4, 100):\n",
            "            with T.block(\"T_add\"):\n",
            "                ax0, ax1 = T.axis.remap(\"SS\", [i0, i1])\n",
            "                T.reads(rxplaceholder[ax0, ax1], rxplaceholder_1[0, ax1])\n",
            "                T.writes(T_add[ax0, ax1])\n",
            "                T_add[ax0, ax1] = rxplaceholder[ax0, ax1] + rxplaceholder_1[0, ax1]\n",
            "    \n",
            "    @T.prim_func\n",
            "    def add(rxplaceholder: T.Buffer[(4, 32, 26, 26), \"float32\"], rxplaceholder_1: T.Buffer[(1, 32, 1, 1), \"float32\"], T_add: T.Buffer[(4, 32, 26, 26), \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"add\", \"tir.noalias\": True})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        for i0, i1, i2, i3 in T.grid(4, 32, 26, 26):\n",
            "            with T.block(\"T_add\"):\n",
            "                ax0, ax1, ax2, ax3 = T.axis.remap(\"SSSS\", [i0, i1, i2, i3])\n",
            "                T.reads(rxplaceholder[ax0, ax1, ax2, ax3], rxplaceholder_1[0, ax1, 0, 0])\n",
            "                T.writes(T_add[ax0, ax1, ax2, ax3])\n",
            "                T_add[ax0, ax1, ax2, ax3] = rxplaceholder[ax0, ax1, ax2, ax3] + rxplaceholder_1[0, ax1, 0, 0]\n",
            "    \n",
            "    @T.prim_func\n",
            "    def relu1(rxplaceholder: T.Buffer[(4, 100), \"float32\"], compute: T.Buffer[(4, 100), \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"relu1\", \"tir.noalias\": True})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        for i0, i1 in T.grid(4, 100):\n",
            "            with T.block(\"compute\"):\n",
            "                i0_1, i1_1 = T.axis.remap(\"SS\", [i0, i1])\n",
            "                T.reads(rxplaceholder[i0_1, i1_1])\n",
            "                T.writes(compute[i0_1, i1_1])\n",
            "                compute[i0_1, i1_1] = T.max(rxplaceholder[i0_1, i1_1], T.float32(0))\n",
            "    \n",
            "    @T.prim_func\n",
            "    def relu(rxplaceholder: T.Buffer[(4, 32, 26, 26), \"float32\"], compute: T.Buffer[(4, 32, 26, 26), \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"relu\", \"tir.noalias\": True})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        for i0, i1, i2, i3 in T.grid(4, 32, 26, 26):\n",
            "            with T.block(\"compute\"):\n",
            "                i0_1, i1_1, i2_1, i3_1 = T.axis.remap(\"SSSS\", [i0, i1, i2, i3])\n",
            "                T.reads(rxplaceholder[i0_1, i1_1, i2_1, i3_1])\n",
            "                T.writes(compute[i0_1, i1_1, i2_1, i3_1])\n",
            "                compute[i0_1, i1_1, i2_1, i3_1] = T.max(rxplaceholder[i0_1, i1_1, i2_1, i3_1], T.float32(0))\n",
            "    \n",
            "    @T.prim_func\n",
            "    def dense(rxplaceholder: T.Buffer[(4, 5408), \"float32\"], rxplaceholder_1: T.Buffer[(100, 5408), \"float32\"], T_matmul_NT: T.Buffer[(4, 100), \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"dense\", \"tir.noalias\": True, \"layout_free_buffers\": [1]})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        for i0, i1, i2 in T.grid(4, 100, 5408):\n",
            "            with T.block(\"T_matmul_NT\"):\n",
            "                i, j, k = T.axis.remap(\"SSR\", [i0, i1, i2])\n",
            "                T.reads(rxplaceholder[i, k], rxplaceholder_1[j, k])\n",
            "                T.writes(T_matmul_NT[i, j])\n",
            "                with T.init():\n",
            "                    T_matmul_NT[i, j] = T.float32(0)\n",
            "                T_matmul_NT[i, j] = T_matmul_NT[i, j] + rxplaceholder[i, k] * rxplaceholder_1[j, k]\n",
            "    \n",
            "    @R.function\n",
            "    def main(x: Tensor((4, 1, 28, 28), \"float32\")) -> Tensor(None, \"float32\", ndim = 2):\n",
            "        # block 0\n",
            "        with R.dataflow():\n",
            "            lv = R.call_tir(conv2d, (x, meta[relay.Constant][0]), (4, 32, 26, 26), dtype=\"float32\")\n",
            "            lv1 = R.call_tir(add, (lv, meta[relay.Constant][1]), (4, 32, 26, 26), dtype=\"float32\")\n",
            "            lv2 = R.call_tir(relu, (lv1,), (4, 32, 26, 26), dtype=\"float32\")\n",
            "            lv3 = R.call_tir(pool2d, (lv2,), (4, 32, 13, 13), dtype=\"float32\")\n",
            "            lv4 = R.call_tir(flatten, (lv3,), (4, 5408), dtype=\"float32\")\n",
            "            lv5 = R.call_tir(dense, (lv4, meta[relay.Constant][2]), (4, 100), dtype=\"float32\")\n",
            "            lv6 = R.call_tir(add1, (lv5, meta[relay.Constant][3]), (4, 100), dtype=\"float32\")\n",
            "            lv7 = R.call_tir(relu1, (lv6,), (4, 100), dtype=\"float32\")\n",
            "            lv8 = R.call_tir(dense1, (lv7, meta[relay.Constant][4]), (4, 10), dtype=\"float32\")\n",
            "            lv9 = R.call_tir(add2, (lv8, meta[relay.Constant][5]), (4, 10), dtype=\"float32\")\n",
            "            lv10 = R.call_tir(softmax, (lv9,), (4, 10), dtype=\"float32\")\n",
            "            gv: Tensor((4, 10), \"float32\") = lv10\n",
            "            R.output(gv)\n",
            "        return gv\n",
            "    \n",
            "    @T.prim_func\n",
            "    def flatten(rxplaceholder: T.Buffer[(4, 32, 13, 13), \"float32\"], compute: T.Buffer[(4, 5408), \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"flatten\", \"tir.noalias\": True})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        for i0, i1 in T.grid(4, 5408):\n",
            "            with T.block(\"compute\"):\n",
            "                i, j = T.axis.remap(\"SS\", [i0, i1])\n",
            "                T.reads(rxplaceholder[i, j % 5408 // 169, j % 169 // 13, j % 13])\n",
            "                T.writes(compute[i, j])\n",
            "                compute[i, j] = rxplaceholder[i, j % 5408 // 169, j % 169 // 13, j % 13]\n",
            "    \n",
            "    @T.prim_func\n",
            "    def conv2d(rxplaceholder: T.Buffer[(4, 1, 28, 28), \"float32\"], rxplaceholder_1: T.Buffer[(32, 1, 3, 3), \"float32\"], conv2d_nchw: T.Buffer[(4, 32, 26, 26), \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"conv2d\", \"tir.noalias\": True})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        for i0_0_i1_0_i2_0_i3_0_fused in T.parallel(2704):\n",
            "            for i0_1_i1_1_fused_init in T.unroll(8):\n",
            "                for i2_1_i3_1_fused_init in T.vectorized(4):\n",
            "                    with T.block(\"conv2d_nchw_init\"):\n",
            "                        nn = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_fused // 1352 * 2 + i0_1_i1_1_fused_init // 4)\n",
            "                        ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused % 1352 // 169 * 4 + i0_1_i1_1_fused_init % 4)\n",
            "                        yy = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 169 // 13 * 2 + i2_1_i3_1_fused_init // 2)\n",
            "                        xx = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 13 * 2 + i2_1_i3_1_fused_init % 2)\n",
            "                        T.reads()\n",
            "                        T.writes(conv2d_nchw[nn, ff, yy, xx])\n",
            "                        conv2d_nchw[nn, ff, yy, xx] = T.float32(0)\n",
            "            for i4, i5, i6 in T.grid(1, 3, 3):\n",
            "                for i0_1_i1_1_fused in T.unroll(8):\n",
            "                    for i2_1_i3_1_fused in T.vectorized(4):\n",
            "                        with T.block(\"conv2d_nchw_update\"):\n",
            "                            nn = T.axis.spatial(4, i0_0_i1_0_i2_0_i3_0_fused // 1352 * 2 + i0_1_i1_1_fused // 4)\n",
            "                            ff = T.axis.spatial(32, i0_0_i1_0_i2_0_i3_0_fused % 1352 // 169 * 4 + i0_1_i1_1_fused % 4)\n",
            "                            yy = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 169 // 13 * 2 + i2_1_i3_1_fused // 2)\n",
            "                            xx = T.axis.spatial(26, i0_0_i1_0_i2_0_i3_0_fused % 13 * 2 + i2_1_i3_1_fused % 2)\n",
            "                            rc, ry, rx = T.axis.remap(\"RRR\", [i4, i5, i6])\n",
            "                            T.reads(conv2d_nchw[nn, ff, yy, xx], rxplaceholder[nn, rc, yy + ry, xx + rx], rxplaceholder_1[ff, rc, ry, rx])\n",
            "                            T.writes(conv2d_nchw[nn, ff, yy, xx])\n",
            "                            conv2d_nchw[nn, ff, yy, xx] = conv2d_nchw[nn, ff, yy, xx] + rxplaceholder[nn, rc, yy + ry, xx + rx] * rxplaceholder_1[ff, rc, ry, rx]\n",
            "    \n",
            "    @T.prim_func\n",
            "    def pool2d(rxplaceholder: T.Buffer[(4, 32, 26, 26), \"float32\"], tensor: T.Buffer[(4, 32, 13, 13), \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"pool2d\", \"tir.noalias\": True})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        for i0, i1, i2, i3, i4, i5 in T.grid(4, 32, 13, 13, 2, 2):\n",
            "            with T.block(\"tensor\"):\n",
            "                ax0, ax1, ax2, ax3, rv0, rv1 = T.axis.remap(\"SSSSRR\", [i0, i1, i2, i3, i4, i5])\n",
            "                T.reads(rxplaceholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])\n",
            "                T.writes(tensor[ax0, ax1, ax2, ax3])\n",
            "                with T.init():\n",
            "                    tensor[ax0, ax1, ax2, ax3] = T.float32(-3.4028234663852886e+38)\n",
            "                tensor[ax0, ax1, ax2, ax3] = T.max(tensor[ax0, ax1, ax2, ax3], rxplaceholder[ax0, ax1, ax2 * 2 + rv0, ax3 * 2 + rv1])\n",
            "    \n",
            "    @T.prim_func\n",
            "    def softmax(rxplaceholder: T.Buffer[(4, 10), \"float32\"], T_softmax_norm: T.Buffer[(4, 10), \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"softmax\", \"tir.noalias\": True})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        T_softmax_maxelem = T.alloc_buffer([4], dtype=\"float32\")\n",
            "        T_softmax_exp = T.alloc_buffer([4, 10], dtype=\"float32\")\n",
            "        T_softmax_expsum = T.alloc_buffer([4], dtype=\"float32\")\n",
            "        for i0, i1 in T.grid(4, 10):\n",
            "            with T.block(\"T_softmax_maxelem\"):\n",
            "                i0_1, k = T.axis.remap(\"SR\", [i0, i1])\n",
            "                T.reads(rxplaceholder[i0_1, k])\n",
            "                T.writes(T_softmax_maxelem[i0_1])\n",
            "                with T.init():\n",
            "                    T_softmax_maxelem[i0_1] = T.float32(-3.4028234663852886e+38)\n",
            "                T_softmax_maxelem[i0_1] = T.max(T_softmax_maxelem[i0_1], rxplaceholder[i0_1, k])\n",
            "        for i0, i1 in T.grid(4, 10):\n",
            "            with T.block(\"T_softmax_exp\"):\n",
            "                i0_2, i1_1 = T.axis.remap(\"SS\", [i0, i1])\n",
            "                T.reads(rxplaceholder[i0_2, i1_1], T_softmax_maxelem[i0_2])\n",
            "                T.writes(T_softmax_exp[i0_2, i1_1])\n",
            "                T_softmax_exp[i0_2, i1_1] = T.exp(rxplaceholder[i0_2, i1_1] - T_softmax_maxelem[i0_2], dtype=\"float32\")\n",
            "        for i0_3, i1 in T.grid(4, 10):\n",
            "            with T.block(\"T_softmax_expsum\"):\n",
            "                i0_4, k = T.axis.remap(\"SR\", [i0_3, i1])\n",
            "                T.reads(T_softmax_exp[i0_4, k])\n",
            "                T.writes(T_softmax_expsum[i0_4])\n",
            "                with T.init():\n",
            "                    T_softmax_expsum[i0_4] = T.float32(0)\n",
            "                T_softmax_expsum[i0_4] = T_softmax_expsum[i0_4] + T_softmax_exp[i0_4, k]\n",
            "        for i0_5, i1 in T.grid(4, 10):\n",
            "            with T.block(\"T_softmax_norm\"):\n",
            "                i0_6, i1_2 = T.axis.remap(\"SS\", [i0_5, i1])\n",
            "                T.reads(T_softmax_exp[i0_6, i1_2], T_softmax_expsum[i0_6])\n",
            "                T.writes(T_softmax_norm[i0_6, i1_2])\n",
            "                T.block_attr({\"axis\":1})\n",
            "                T_softmax_norm[i0_6, i1_2] = T_softmax_exp[i0_6, i1_2] / T_softmax_expsum[i0_6]\n",
            "    \n",
            "    @T.prim_func\n",
            "    def add2(rxplaceholder: T.Buffer[(4, 10), \"float32\"], rxplaceholder_1: T.Buffer[(1, 10), \"float32\"], T_add: T.Buffer[(4, 10), \"float32\"]) -> None:\n",
            "        # function attr dict\n",
            "        T.func_attr({\"global_symbol\": \"add2\", \"tir.noalias\": True})\n",
            "        # body\n",
            "        # with T.block(\"root\")\n",
            "        for i0, i1 in T.grid(4, 10):\n",
            "            with T.block(\"T_add\"):\n",
            "                ax0, ax1 = T.axis.remap(\"SS\", [i0, i1])\n",
            "                T.reads(rxplaceholder[ax0, ax1], rxplaceholder_1[0, ax1])\n",
            "                T.writes(T_add[ax0, ax1])\n",
            "                T_add[ax0, ax1] = rxplaceholder[ax0, ax1] + rxplaceholder_1[0, ax1]\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "mod = create_model_via_emit_te()\n",
        "sch = tvm.tir.Schedule(mod)\n",
        "\n",
        "# Step 1. Get blocks\n",
        "block = sch.get_block(name=\"root\", func_name=\"conv2d\")\n",
        "\n",
        "# Step 2. Inline the padding block (if exists)\n",
        "pad_temp = sch.get_block(\"pad_temp\", \"conv2d\")\n",
        "sch.compute_inline(pad_temp)\n",
        "\n",
        "# Step 3. Get loops\n",
        "conv = sch.get_block(\"conv2d_nchw\",\"conv2d\")\n",
        "\n",
        "# Step 4. Organize the loops\n",
        "# https://github.com/mlc-ai/mlc-zh/discussions/80\n",
        "i0, i1, i2, i3, i4, i5, i6 = sch.get_loops(conv)\n",
        "\n",
        "i0_0, i0_1 = sch.split(i0, factors=[2, 2])\n",
        "i1_0, i1_1 = sch.split(i1, factors=[None, 4])\n",
        "i2_0, i2_1 = sch.split(i2, factors=[None, 2])\n",
        "i3_0, i3_1 = sch.split(i3, factors=[None, 2])\n",
        "sch.reorder(i0_0, i1_0, i2_0, i3_0, i4, i5, i6, i0_1, i1_1, i2_1, i3_1)\n",
        "\n",
        "\n",
        "i0_0, i1_0, i2_0, i3_0, i4, i5, i6, i0_1, i1_1, i2_1, i3_1 = sch.get_loops(conv)\n",
        "\n",
        "sch.fuse(i0_0, i1_0, i2_0, i3_0)\n",
        "i0_0_i1_0_i2_0_i3_0_fuse, i4, i5, i6, i0_1, i1_1, i2_1, i3_1 = sch.get_loops(conv)\n",
        "\n",
        "sch.parallel(i0_0_i1_0_i2_0_i3_0_fuse)\n",
        "\n",
        "sch.fuse(i0_1,i1_1)\n",
        "sch.fuse(i2_1,i3_1)\n",
        "\n",
        "\n",
        "i0_0_i1_0_i2_0_i3_0_fuse, i4, i5, i6, i0_1_i1_1_fused, i2_1_i3_1_fused = sch.get_loops(conv)\n",
        "sch.unroll(i0_1_i1_1_fused)\n",
        "sch.vectorize(i2_1_i3_1_fused)\n",
        "\n",
        "# Step 5. decompose reduction\n",
        "sch.decompose_reduction(conv, i4)\n",
        "\n",
        "# Step 6. fuse + vectorize / fuse + parallel / fuse + unroll\n",
        "\n",
        "# IPython.display.Code(sch.mod.script(), language=\"python\")\n",
        "print(sch.mod.script())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcnzai9JKJ_w"
      },
      "source": [
        "同样，我们可以测试变换后IRModule的正确性。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Lvc4pxghKJ_w"
      },
      "outputs": [],
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "check_equivalence(sch.mod, torch_model, test_loader)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    },
    "colab": {
      "name": "mlc_a2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}